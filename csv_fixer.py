"""
script to fix the CSVs generated by the modularities analysis because
they're generated with spaces for delimiters but contain a list of hashtags that have spaces between
therefore the csv is totally screwed, so we need to repackage them to fix that
also, we repackage them into a single giant csv file for much easier usage
"""
import glob
import os
import pandas as pd

def main():
    """
    script to fix the CSVs generated by the modularities analysis because they're generated 
    with spaces for delimiters but contain a list of hashtags that have spaces between
    therefore the csv is totally screwed, so we need to repackage them to fix that
    also, we repackage them into a single giant csv file for much easier usage
    """
    # get files
    csv_files = glob.glob(os.path.join("data/2_hashtag_stbm/", '*.csv'))
    with open("data/collocations/2_hashtag_modularities_nodes_1000plus.csv", "r", encoding="utf-8") as mod_handle:
        # open nodes csv
        hashtag_modularities = pd.read_csv(mod_handle, delimiter=" ", index_col="ID")
    # set up loop variables
    df_list = []
    total_hits = 0
    total_misses = 0
    for file in csv_files:
        with open(file, "r", encoding="utf-8") as csv_handle:
            modularities = []
            # read the file
            csv_as_df = pd.read_csv(csv_handle, delimiter=" ")
            # fix the problem
            csv_as_df["hashtags"] = csv_as_df.hashtags.apply(lambda x: x[1:-1].replace("'", "").replace('"', "").split(', '))
            for _, row in csv_as_df.iterrows():
                # for each row, determine its overall modularity
                # each row has multiple hashtags that might be in
                # different groups, so we collapse into one
                row_mod, hits, misses = row_modularity(row["hashtags"], hashtag_modularities)
                total_hits += hits
                total_misses += misses
                modularities.append(row_mod)
            csv_as_df["modularity"] = modularities
            df_list.append(csv_as_df)

    print(f"Total hits: {total_hits}")
    print(f"Total misses: {total_misses}")
    print(f"Ratio: {total_hits/total_misses}")

    big_df = pd.concat(df_list)
    big_df.to_csv("data/2_H_STBM_MASTER.csv", sep=";", encoding="utf-8", index=False)

def row_modularity(row_hashtags, hashtag_df):
    """
    Determine the overall modularity of a tweet/post
    Not actually used in final analysis

    Args:
        row_hashtags (list): List of hashtags in the row
        hashtag_df (pd.df): _description_

    Returns:
        int: row modularity
        int: number of hits in given data
        int: number of misses in given data
    """
    # set up loop variables
    mods_list = []
    hit_count = 0
    fail_count = 0
    for hashtag in row_hashtags:
        try:
            mods_list.append(hashtag_df.at[hashtag, "modularity_class"])
            hit_count += 1
        except KeyError:
            # keyerrors can happen when a hashtag appears that does
            # not appear at least 999 times across the whole dataset
            # we don't have to do anything except remember this and ignore them
            fail_count += 1

    # the overall modularity of the row is the most common one appearing in the hashtags
    return max(mods_list, key=mods_list.count), hit_count, fail_count

if __name__ == '__main__':
    main()
