"""
script to fix the CSVs generated by the modularities analysis because
they're generated with spaces for delimiters but contain a list of hashtags that have spaces between
therefore the csv is totally screwed, so we need to repackage them to fix that
also, we repackage them into a single giant csv file :)
"""
import sys 
import glob
import os
import pandas as pd

def main(args):
    csv_files = glob.glob(os.path.join("data/2_hashtag_stbm/", '*.csv'))
    with open("data/collocations/2_hashtag_modularities_nodes_1000plus.csv", "r", encoding="utf-8") as mod_handle:
        hashtag_modularities = pd.read_csv(mod_handle, delimiter=" ", index_col="ID")
        #hashtag_modularities["ID"] = hashtag_modularities.ID.apply(lambda x: str(x))
    #print(hashtag_modularities)
    df_list = []
    total_hits = 0
    total_misses = 0
    for file in csv_files:
        with open(file, "r", encoding="utf-8") as csv_handle:
            modularities = []
            csv_as_df = pd.read_csv(csv_handle, delimiter=" ")
            csv_as_df["hashtags"] = csv_as_df.hashtags.apply(lambda x: x[1:-1].replace("'", "").replace('"', "").split(', '))
            for _, row in csv_as_df.iterrows():
                row_mod, hits, misses = row_modularity(row["hashtags"], hashtag_modularities)
                total_hits += hits
                total_misses += misses
                modularities.append(row_mod)
            csv_as_df["modularity"] = modularities
            df_list.append(csv_as_df)
    
    print(f"Total hits: {total_hits}")
    print(f"Total misses: {total_misses}")
    print(f"Ratio: {total_hits/total_misses}")

    big_df = pd.concat(df_list)
    big_df.to_csv("data/2_H_STBM_MASTER.csv", sep=";", encoding="utf-8", index=False)

def row_modularity(row_hashtags, hashtag_df):
    mods_list = []
    hit_count = 0
    fail_count = 0
    for hashtag in row_hashtags:
        #print(hashtag)
        try:
            mods_list.append(hashtag_df.at[hashtag, "modularity_class"])
            hit_count += 1
        except KeyError:
            # keyerrors can happen when a hashtag appears that does 
            # not appear at least 999 times across the whole dataset
            # we don't have to do anything except remember this and ignore them
            fail_count += 1
    
    # the overall modularity of the row is the most common one appearing in the hashtags
    return max(mods_list, key=mods_list.count), hit_count, fail_count
    
    
if __name__ == '__main__':
    main(sys.argv[1:])